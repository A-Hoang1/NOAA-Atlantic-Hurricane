---
title: "Airbnb prices Final Project"
author: "Andrew & Ben"
output: html_document
date: "2024-11-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
 fig.align = "center")
 #message = F,
 #warning = F)

# Loading in the needed packages
pacman::p_load(tidyverse, skimr, GGally, regclass, broom, caTools, FNN, rpart, rpart.plot)

# Changing default themes
theme_set(theme_bw())
theme_update(plot.title = element_text(hjust = 0.5),
 plot.subtitle = element_text(hjust = 0.5))

# Changing the default choice for how many decimal places are displayed
options(digits = 4)
```

# Introduction
Airbnb is a popular company that connects people looking for accommodations with hosts who offer lodging options. The company was founded in 2008 and allows hosts to list their own properties, which can range from a single room to an entire house) for short-term rentals. Even though Airbnb has fares that it sets, most of the price comes down to what the host want to set the price at. This inspired us to determine if the number of people that can be accommodated in an Airbnb properties affects the price of the listing. If this is not true, we wanted to explore if there are any other variables that affect the price more than how many people it can accommodate. More information on the additional types of fees that Airbnb charges can be found at: <https://www.airbnb.com/help/article/125>. The data was retrieved from Kaggle user Oscar Batiz the link is the following: <https://www.kaggle.com/datasets/oscarbatiz/los-angeles-airbnb-listings>.

## Getting the data
The code chunk below will load the data of Airbnb listings as of 04 of September 2024. We'll start by cleaning the data and removing any columns that do not have information. 
```{r}
# Reads in the file
listings <- read.csv("listings.csv")

# Clean up the data
listings_clean <- 
  listings |> 
  # Selects the columns that we are interested in exploring
  dplyr::select(price, room_type:beds) |>
  
  # Removes listings without a price
  filter(!is.na(price))

# Displays clean data
str(listings_clean)
```

The variables we are going to use are: 

1) **price**: The price of the AirBnb in US dollars. 

2) **room_type**: Is used to describe the type of listing (i.e: if the listings is a "Entire home/apt, "Private Room", "Shared Room" or "Hotel Room")

3) **accommodates**: Is the number of people that the listing can accommodate

4) **bathrooms**: Is the number of bathrooms the listings has

5) **bedrooms**: Is the number of bedrooms the listing has

6) **beds**: Is the number of beds the listing has



## Preparing the Data
```{r}
# Displays a quartile summary for the data
summary(listings_clean$price)
```

There seems to be some outliers in this data, particularly with the maximum price of $56,425 which is much higher than the third quartile of 260. This is causing the mean (289) to be notably higher than the median (155) indicating a skewed distribution.

Lets try to fix this by removing some outliers:
```{r}
# Calculate the 5th and 95th percentiles for the price variable
lower_5th <- quantile(listings_clean$price, 0.05, na.rm = TRUE)
upper_95th <- quantile(listings_clean$price, 0.95, na.rm = TRUE)

# Filters the dataset to keep only the prices within the 5th & 95th percentiles
listings_clean2 <- 
  listings_clean |> 
  filter(price >= lower_5th & price <= upper_95th)

# Checks the summary of the new dataset without extreme outliers
summary(listings_clean2$price)
```

Looking at our data now, there seems to be fewer outliers now since we have taken out the top 95th and bottom 5th percentiles to get rid of extreme cases. These could be either fake listings or listings that are very rare so we do not want to include these in our models further on. Our median is now closer to our mean and our max and min are not as far away from our mean as well.

Lets have a look at our histogram of price:
```{r}
# Creates a plot of price
ggplot(data = listings_clean2, 
       mapping = aes(x = price)) +
  
  # Plots the histogram
  geom_histogram(bins = 35, 
                 fill = "steelblue", 
                 color = "black") +
  
  # Changes titles
  labs(title = "Distribution of Airbnb Prices",
       x = "Price", 
       y = "Frequency") + 
  
  # Adds currency to the x-axis labels
  scale_x_continuous(label = scales::label_currency(),
                     expand = c(0, 0, 0.05, 0)) + 
  scale_y_continuous(expand = c(0, 0, 0.05, 0))

```

Looking at our histogram there still seems to be a noticeble right-skew, with a concentration of values between approximately 60 and 300. This suggests, that while most listings are priced lower, a few high-priced outliers are influencing the ditribution, by pulling the mean upwards. 

Lets try applying a log transformation to the price to see if this helps.
```{r}
# Applies log10 transformation to the listing
listings_clean2$log10_price <- log10(listings_clean2$price)

```
Now that we have applied a log10 transformation to our data let's have a look to see if the data follows a more normal distribution.

```{r}
# Creates a plot of price
ggplot(data = listings_clean2, 
       mapping = aes(x = price)) +
  
  # Plots a histogram
  geom_histogram(bins = 35, 
                 fill = "steelblue", 
                 color = "black") +
  
  # Changes the labels
  labs(title = "Distribution of Airbnb Prices",
       x = "Price", 
       y = "Frequency") + 
  
  # Adds a log10 transformation and currency to label on x-axis
  scale_x_continuous(label = scales::label_currency(),
                     trans = scales::log10_trans(),
                     expand = c(0, 0, 0.05, 0)) + 
  scale_y_continuous(expand = c(0, 0, 0.05, 0))

```

Now that we have applied the transformation our data looks more normal and it is not as skewed.

Lets explore the room types variable to see if this will help predict the price of the Airbnbs.
```{r}
# Creates a plot for room_types
ggplot(data = listings |> filter(!is.na(price) & !is.na(room_type)),
       mapping = aes(
         x = as.factor(room_type), 
         y = price 
       )
) +
  
  # Plots the boxplot
  geom_boxplot(fill = "steelblue") +
   
  # Changes label titles 
  labs(
    title = "Airbnb Prices by Room Type",
    x = "Room Type", 
    y = "Price"
  ) + 
  
  # Changes labels to currency and adds a log10 transformation
  scale_y_continuous(label = scales::label_currency(),
                     trans = scales::log10_trans()) 

```

Since the price range for entire homes/apartments is comparable to hotels and there are only 248 listings for hotel rooms, and the price range for private and shared rooms (only 747 listings of shared room type) is similar, it makes sense to group these categories into two broader groups for analysis. This approach simplifies the data by grouping similar categories, creating one group for “Rooms” (private and shared rooms), which will be assigned a "0", and another for “Hotels Rooms/Entire Apartments/Homes”, which will be assigned a 1. This categorization will help us generalize insights into pricing trends across different accommodations.

Lets go ahead and group these types of room type listings.
```{r}
listings_clean2 <- 
  listings_clean2 |> 
  mutate(
    # Changes "Entire home/apt" and "Hotel Room" to "1"
    # Changes "Private room" and "Shared room" to "0"
    room_type = if_else(
      room_type == "Entire home/apt" | room_type == "Hotel Room", 1, 0
    )
  )
```

Lets have a look at the boxplot for the new categorization of listings for predicting price.
```{r}
# Creates a plot for room_types
ggplot(data = listings_clean2,
       mapping = aes(
         x = as.factor(room_type), 
         y = price
       )
) +
  
  # Plots the boxplot
  geom_boxplot(fill = "steelblue") +
   
  # Changes label titles 
  labs(
    title = "Airbnb Prices by Room Type",
    x = "Room Type", 
    y = "Price"
  ) + 
  
  # Changes labels to currency and adds a log10 transformation
  scale_y_continuous(label = scales::label_currency(),
                     trans = scales::log10_trans()) + 
  
  # Relabels x-axis so that it is not a binary variable
  scale_x_discrete(labels = c(
    "0" = "Private or Shared Rooms",
    "1" = "Entire Homes, Apt & Hotel Rooms"
  ))

```

Looking at the Airbnb room type boxplots the presence of outliers in the higher range for entire homes indicates while most of the listings in this category are within are certain price range, there seems to be a few luxury listings contributing significantly to inflating the overall price average. The median listing for "Entire home/apt" or "Hotel rooms" seems to be higher than "Private rooms" or "Shared rooms". Additionally, the distribution of price for entire homes/apt or hotel rooms seems to be much higher than for private or shared rooms.

Now that we changed this variable we will now longer need the large listings dataset so we can remove it. 

```{r}
# Removes the listings data set since it is large and not used again
rm(listings)
```

Lets explore some of the other predictors before we start building some of our models.
```{r}
listings_clean2 |> 
  drop_na() |> 
  dplyr::select(where(is.numeric)) |> 
  ggcorr(
    low = "tomato",
    mid = "white",
    high = "steelblue",
    label = T,
    label_round = 2,
    angle = -25,
    hjust = 1
  )

```

There seems to be some potential multicollinearity, especially among bedrooms, beds, bathrooms and accommodates. Using all these variables could potentially lead to misinterpretation of our predictors and could also inflate the standard errors by making it difficult to assess the significance of individual predictors in the model.

Lets have a look at the variables with the highest association with listings price.
```{r}
summary(listings_clean2$accommodates)

summary(listings_clean2$bedrooms)
```
The maximum value of 50 is significantly larger than the rest of the data, with the median of 1 and the third quartile of 2. This extreme value of 50 suggests the presence of some potential outlier in the data. 

Lets go ahead and remove the highest and lowest percentile of bedrooms.
```{r}
# Calculate the 1st and 99th percentiles for the bedrooms variable
lower_percentile <- quantile(listings_clean2$bedrooms, 0.01, na.rm = TRUE)
upper_99th <- quantile(listings_clean2$bedrooms, 0.99, na.rm = TRUE)

# Filter the dataset to keep only the listings within the percentiles
listings_clean3 <- listings_clean2 |> 
  filter(bedrooms >= lower_percentile & bedrooms <= upper_99th)

# Check the summary of the new dataset without extreme outliers
summary(listings_clean3$bedrooms)
```
Now the furthest percentile outliers have been removed. Lets continue exploring our predictors.


Since both the bedrooms and accommodates have a high association with price. Lets look at these predictors in a graph.
```{r}
listings_clean3 |> 
  # Removes rows with NA values
  filter(!is.na(accommodates) & !is.na(bedrooms)) |> 
  # Adds both predictors we want to explore to single column
  pivot_longer(
    cols = c(accommodates, bedrooms), 
    names_to = "variable",
    values_to = "values"
  ) |> 
  
  # Plots values to x axis and price to y
  ggplot(
    mapping = aes(
      x = values, 
      y = price)) +
  
  # Adds title and removes axis labels
  labs(title = "Predicting Price with Different Variables",
       x = NULL, 
       y = NULL) +
  
  # Adds points to plot
  geom_point() +
  
  # Changes y labels to currency
  scale_y_continuous(label = scales::label_currency(),
                     trans = scales::log10_trans()) +
  
  # Creates separate graph for each of the numeric variables
  facet_wrap(
    facets = ~variable,
    scales = "free_x", 
    nrow = 2
  )  +

  # Adds trends line to graph
  geom_smooth(
    color = "red",
    method = "loess",
    formula = y~x,
    se = F
  ) 
```

Accommodates, seems to have a nonlinear relationship with price, where prices increase with the more people the AirBnb can accommodate, but plateau at higher values. This suggests diminishing price as the number of people a listing can accommodate grows. For bedrooms, the relationship with price appears more linear, with prices steadily increasing as the number of bedrooms rises. This indicates that bedrooms may be a stronger and more consistent predictor of price compared to accommodates, particularly for higher-priced listings. Now that we know this lets start working on some machine learning models with our data.


# Machine Learning Techniques
Lets start by constructing some Linear Models to determine price.
```{r}
listings_clean3 <- listings_clean3 |> drop_na()
# Simple model predicting price based on accommodates only
price_lm1 <- lm(
  formula = price ~ accommodates,
  data = listings_clean3
)

# Simple model predicting price based on bedrooms only
price_lm2 <- lm(
  formula = price ~ bedrooms,
  data = listings_clean3
)

# Adds room_type to the predictors
price_lm3 <- lm(
  formula = price ~ accommodates + room_type,
  data = listings_clean3
)

# Predicts price using bedrooms and room_type
price_lm4 <- lm(
  formula = price ~ bedrooms + room_type,
  data = listings_clean3
)

# Combines bedrooms, room_type, and accommodates as predictors
price_lm5 <- lm(
  formula = price ~ bedrooms + room_type + accommodates,
  data = listings_clean3
)

# Uses bedrooms, room_type, and bathrooms to predict price
price_lm6 <- lm(
  formula = price ~ bedrooms + room_type + bathrooms,
  data = listings_clean3
)

# Adds accommodates to price_lm6 predictors
price_lm7 <- lm(
  formula = price ~ bedrooms + room_type + accommodates + bathrooms,
  data = listings_clean3
)
```
Now that we have built some models lets analyze which model is the best at predicting Airbnb prices.

Analyzing our Models
```{r}
bind_rows(
  .id = "model",
  "price_lm1" = glance(price_lm1),
  "price_lm2" = glance(price_lm2),
  "price_lm3" = glance(price_lm3),
  "price_lm4" = glance(price_lm4),
  "price_lm5" = glance(price_lm5),
  "price_lm6" = glance(price_lm6),
  "price_lm7" = glance(price_lm7)
) |> 
  dplyr::select(model, n_predictors = df, r.squared, sigma) |> 
  mutate(r.squared = round(r.squared, 3),
         sigma = round(sigma, 0),
         MAE = round(c(
           mean(abs(price_lm1$residuals)),
           mean(abs(price_lm2$residuals)),
           mean(abs(price_lm3$residuals)),
           mean(abs(price_lm4$residuals)),
           mean(abs(price_lm5$residuals)),
           mean(abs(price_lm6$residuals)),
           mean(abs(price_lm7$residuals))), 3)) |> 
  gt::gt()

```
Overall, price_lm6 appears to be the best trade-off between simplicity and performance. It has the second-highest R-squared (0.440), the lowest sigma (107), and uses only three predictors (bedrooms, room_type, bathrooms), making it simpler and less prone to overfitting compared to price_lm7. The sigma still seems to be high at 107. This number means that typically, the model’s predictions of Airbnb prices are off by approximately \$107 from the actual prices which is quite high. The average prediction error looking at MAE is 71.61, which means the average prediction error is off by \$71.61.

Lets have a further look if these predictors will be a problem with our model by calculating the variance inflation factor (VIF). 
```{r}
regclass::VIF(price_lm6)
```
Since our VIF values are below 5, it indicates that multicollinearity is not severe.


```{r}
# Predictions for the test dataset using the full data frame as newdata
price7 <- predict(object = price_lm7, newdata = listings_clean3)
price6 <- predict(object = price_lm6, newdata = listings_clean3)
price5 <- predict(object = price_lm5, newdata = listings_clean3)
price4 <- predict(object = price_lm4, newdata = listings_clean3)
price3 <- predict(object = price_lm3, newdata = listings_clean3)
price2 <- predict(object = price_lm2, newdata = listings_clean3)
price1 <- predict(object = price_lm1, newdata = listings_clean3)

# Create a data frame with actual prices and predictions from all models
price_pred <- data.frame(
  actual_price = listings_clean3$price,
  price7 = price7,
  price6 = price6,
  price5 = price5,
  price4 = price4,
  price3 = price3,
  price2 = price2,
  price1 = price1
)

head(price_pred)

```
It seems the predictions are a bit off, let's try and graph the relationship between actual and predicted prices to better visualize the model's performance.

```{r}
# residual plot
augment_columns(
  x = price_lm6,
  data = listings_clean3
) |> 
  dplyr::select(bedrooms, room_type, bathrooms, .resid) |> 
  # Pivoting the 3 predictors into one column
  pivot_longer(
    cols = -.resid,
    names_to = "predictor",
    values_to = "value"
  ) |> 
  # Ordering them in the same order as the data
  mutate(predictor = as_factor(predictor)) |> 
  
  # Creating the individual residual plots
  ggplot(
    mapping = aes(
      x = value,
      y = .resid
    )
  ) +
  
  geom_point(alpha = 0.25) + 
  
  # Adding a horizontal line at y = 0
  geom_hline(
    mapping = aes(yintercept = mean(.resid)),
    color = "red",
    linewidth = 1
  ) +
  
  # Adding a blue trend line that should be very similar to the red line
  # if there isn't a non-linear trend between x & y
  geom_smooth(
    method = "loess",
    se = F,
    formula = y ~ x,
    color = "steelblue",
    linewidth = 1
  ) +
  
  # A residual plot for the 3 predictors
  facet_wrap(
    facets = vars(predictor),
    scales = "free_x",
    nrow = 5
  ) + 
  # Changing the labels and adding $ to the y-axis
  labs(
    x = NULL,
    y = "Residuals"
  ) + 
  scale_y_continuous(labels = scales::label_dollar()) 
```

The residual plots show consistent model performance for bedrooms and room_type, with residuals centered around $0 and no significant trends. However, for bathrooms, residuals show a clear downward trend as the number of bathrooms increases, indicating the model underestimates prices for listings with many bathrooms.

---------------------------------------------------------------------------------------------------------------------------
Since our linear models were not great at predicting the price of the listings lets go ahead and try some other methods, but first let's normalize and standardize our data before applying other methods.
```{r}
# Normalize function:
normalize <- function(x) {
  norm_x <- (x - min(x)) / (max(x) - min(x))
  return(norm_x)
}

# Normalizing the data
listings_norm <- 
  listings_clean3 |> 
  filter(!is.na(beds) & !is.na(bathrooms)) |>
  mutate(
    across(
      .cols = -price,
      .fns = normalize
    )
  )

skim(listings_norm)

# Standardize function:
standardize <- function(x) {
  standard_x <- (x - mean(x))/sd(x)
  return(standard_x)
}


# Standardizing the data
listings_stan <-
  listings_clean3 |>
  filter(!is.na(beds) & !is.na(bathrooms)) |>
  mutate(
    across(
      .cols = -price, # Standardize the explanatory variables
      .fns = standardize
    )
  )

skim(listings_stan)
```

Since our predictor variables are numeric lets try to use kNN regression to make some predictions for price, with bedrooms, and bathrooms, lets leave room_types out as they are just binary values/categorical. 

```{r}
# Normalizing and standardizing 
clean3_norm <- 
  listings_clean3 |> 
  mutate(
    across(
      .cols = c(bedrooms, bathrooms),
      .fns = normalize))

clean3_stan <- 
  listings_clean3 |> 
  mutate(
    across(
      .cols = c(beds, bathrooms), 
      .fns = standardize))

# Set seed for reproducibility 
set.seed(1234)

# k 1:100 
k <- 1:100

# creating a data.frame fit_stats_norm to store the results in:
fit_stats_norm <-
  tibble(k = k,
         R2 = rep(-1, length(k)),
         MAE = rep(-1, length(k))
    )

train_norm <- clean3_norm |> dplyr::select(bedrooms, bathrooms)

# looping through the results
for (i in 1:length(k)) {
  loop_knn <-
    knn.reg(
      train = train_norm,
      y = listings_clean3$log10_price,
      k = k[i]
    )
  
  fit_stats_norm[i, "R2"] <- loop_knn$R2Pred
  
  fit_stats_norm[i, "MAE"] <- (listings_clean3$log10_price - loop_knn$pred) |> abs() |> mean()
}

# creating a data.frame fit_stats_stan to store the results in:
fit_stats_stan <-
  tibble(k = k,
         R2 = rep(-1, length(k)),
         MAE = rep(-1, length(k))
    )

train_stan <- clean3_stan |> dplyr::select(bedrooms, bathrooms)

# looping through the rsults
for (i in 1:length(k)) {
  loop_knn <-
    knn.reg(
      train = train_stan,
      y = listings_clean3$log10_price,
      k = k[i]
    )
  
  fit_stats_stan[i, "R2"] <- loop_knn$R2Pred
  
  fit_stats_stan[i, "MAE"] <- (listings_clean3$log10_price - loop_knn$pred) |> abs() |> mean()
}

fit_stats_combined <- 
  bind_rows("stan" = fit_stats_stan,
            "norm" = fit_stats_norm,
            .id = "rescale") 


fit_stats_combined |>  

  # Creating one column to store the fit statistics in: Tidy version  
  pivot_longer(cols = c(R2, MAE),
               names_to = "fit_stat",
               values_to = "value") |>  

  ggplot(mapping = aes(x = k,
                       y = value,
                       color = rescale)) + 
  #geom_point() + 
  geom_line() + 
  
  facet_wrap(facets = ~ fit_stat,
             scales = 'free_y',
             ncol = 1) +
  
  labs(y = NULL,
       color = NULL)

fit_stats_combined |>
  filter(
    R2 == max(R2) | MAE == min(MAE)
  )

```

```{r}
# reproducibility
set.seed(1234)

# kNN Regression with k = 48
price_knn48 <- 
  knn.reg(
    train = listings_norm,              
    y = listings_clean3$log10_price,     
    k = 48                              
  )
# saving actual and predicted prices
actual_prices <- listings_clean3$log10_price
predicted_prices <- price_knn48$pred

# display the results and calculate fit statistics
price_df <- tibble(
  y = listings_clean3$log10_price,
  predicted = predicted_prices,
  residuals = y - predicted
) |>
  summarize(
    SSE = sum(residuals^2),
    SST = sum((y-mean(y))^2),
    R2 = 1 - SSE / SST,
    rmse = sqrt(mean(residuals^2)),
    MAE = residuals |> abs() |> mean(),
    MAE_mean = (y - mean(y)) |> abs() |> mean(),
    MARE = 1 - MAE / MAE_mean
    
  )

price_df
```
Taking a look at our fit statistics, we can see that our best choice of k is 48 with an R2 of 0.4421 and MAE of 0.1618. Our model's MAE is lower than the baseline MAE_mean of 0.2208. This indicates that while our model performs better than simply predicting the mean price, it still has significant room for improvement. The RMSE of 0.2042 is relatively close to our MAE, suggesting our errors are fairly consistent without extreme outliers. These statistics suggest that kNN regression isn't great for predicting prices with just bedrooms and bathrooms as predictors. Leaving out room_type, which is a binary variable but also one that contributes significantly in predicting prices, affects the overall variance of our model. Given that our model only explains about 44.21% of the variance in prices, a different modeling approach that can incorporate both numeric and categorical variables (such as a regression tree) might be more appropriate.

Let's look at the relationships between our predictors and price,
```{r}
listings_clean3 |> 
  pivot_longer(
    cols = c(bedrooms, bathrooms, room_type), 
    names_to = "variable",
    values_to = "value"
  ) |> 
  mutate(
    variable = as_factor(variable),
    high_price = if_else(log10_price > 2.5, "red", "black")
  ) |> 
  
  ggplot(
    mapping = aes(
      x = value,
      y = log10_price
    )
  ) + 
  
  geom_point(
    mapping = aes(color = high_price),
    show.legend = F,
    alpha = 0.5
  ) + 
  
  geom_smooth(
    method = "loess",
    se = F,
    formula = y~x
  ) + 
  
  facet_wrap(
    facets = ~ variable,
    scales = "free_x"
  ) + 
  
  labs(
    # x = NULL,
    y = "log10(Price)"
  ) + 
  
  scale_color_identity() +
  scale_y_continuous(trans = scales::log10_trans())
  
```

We can see clear non-linear patterns, particularly in how bathrooms and bedrooms relate to price, as shown by the curving blue lines. Additionally, room_type shows clear categorical effects, however using a regression tree still seems more promising as we will be able to use all variables in our predictions.

Let's create the full tree with our predictors and find the prune cutoff.
```{r}
RNGversion("4.1.0")
set.seed(1234)

# full regression tree with bedrooms, bathrooms, and room_type
airbnb_full <-
 rpart(
   formula = log10_price ~ bedrooms + room_type + bathrooms,
   data = listings_clean3,
   method = "anova",  # indicates to make a reg tree
   # stop the prepruning criteria
   minsplit = 2,
   minbucket = 1,
   cp = 0
 )

# find xerror cutoff
airbnb_full$cptable |>
 data.frame() |>
 # find row with smallest xerror
 slice_min(xerror, n = 1, with_ties = F) |>
 # using mutate xerror + xstd = xerror_cutoff
 mutate(xerror_cutoff = xerror + xstd) |>
 # save the xerror cutoff value as xcutoff
 pull(xerror_cutoff) ->
 xcutoff
 
# find the cp value to prune the tree
airbnb_full$cptable |>
 data.frame() |>
 # use filter to pick the rows less than the cutoff value
 filter(xerror < xcutoff) |>
 # use slice to keep the first row
 slice(1) |>
 # save the cp value
 pull(CP) ->
 prune_cp

c("xcutoff" = xcutoff,
 "CP Prune" = prune_cp) |>
 round(digits = 4)
```

Now that we have our prune cutoff lets prune the full tree and visualize the results.
```{r}
# Prune the tree
prune(tree = airbnb_full,
     cp = prune_cp) ->
 airbnb_tree

# Create visual plot of the pruned tree
rpart.plot(airbnb_tree, 
          digits = 4,
          fallen.leaves = TRUE,
          type = 5,
          box.palette = 'BlGnYl',
          shadow.col = 'gray')
```

The regression tree shows that the variable of importance seems to be the number of bedrooms, with a split at 2 bedrooms. For smaller properties with less than 2 bedrooms, room type and bathrooms are key factors, with fewer bathrooms generally leading to lower prices, as shown by nodes values around 1.8 to 2.2 (log10_prices). For larger properties with greater than or equal to 2 bedrooms, it seems higher bathroom counts are linked to higher prices, reaching values as high as 2.64 for specific combinations of room type and bathroom count. Overall, the highest prices are associated with properties with more bedrooms, bathrooms and the type of room(s) while the lowest prices are found in properties with fewer amenities of variables.

```{r}
# generate predictions for the original listings dataset
predict(
 object = airbnb_tree
) ->
 pred_price

# compare the distribution of predicted vs. actual
bind_rows(
 "predicted" = summary(pred_price),
 "actual" = summary(listings_clean3$log10_price),
 .id = "price"
)

# Look at the correlation of outcomes with predictions
listings_clean3 |>
 mutate(pred_price = pred_price) |>
 summarize(
   "R2" = cor(pred_price, log10_price)^2,
   "MAE" = mean(abs(pred_price - log10_price)),
   "MAE_mean" = mean(abs(log10_price - mean(log10_price)))
 ) |>
 mutate(mae_reduc = 1 - MAE/MAE_mean)
```

From our results of our fit statistics we can see that the our R2 has improved by approximately 0.0894, which is a little better than our kNN regression model (R2 = 0.4421), while our MAE of 0.1464 improved by 0.0164, this indicates our predictions still deviate by about 0.15 (log10) from actual prices, showing a mean reduction in error of 33.71% compared to baseline predictions, with these improvements in mind we should use a regression tree model rather than kNN regression. 

Finally let's look at the actual vs predictions of airbnb prices together.
```{r}
pred_results <- 
 listings_clean3 |>
 mutate(predicted = pred_price,
        node = airbnb_tree$where)

# cases are in each of our nodes
pred_results |>
 count(node) |>
 arrange(desc(n))

# graph of predicted vs actual values
ggplot(
 data = pred_results,
 mapping = aes(
   x = predicted, 
   y = listings_clean3$price
 )
) +
 geom_point(
   mapping = aes(
     color = factor(node)
   ),
   show.legend = F,
   alpha = 0.25
 ) +
 labs(
   x = "Predicted log10_price",
   y = "Actual log10_price",
   title = "Predicted vs Actual Airbnb Prices"
 ) +
  scale_x_continuous(trans = scales::log10_trans()) +
 theme_bw()
```

The spread suggests our model performs better at predicting mid-range prices, while showing more variability at the extreme ends of the price spectrum, particularly for higher-priced listings where predictions tend to underestimate actual prices. 